{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TestBench.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3vzdcbwbsWpl","executionInfo":{"status":"ok","timestamp":1602547101459,"user_tz":-780,"elapsed":930,"user":{"displayName":"Hassaan Mirza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbDdstoeI3uMgQE2uIuTfxDWaRQUODiJ6EY54ABC8=s64","userId":"07902953940759638847"}},"outputId":"70cd93f8-51c8-48b1-ea3c-8f7d07af714d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SEzZRGOXt_7O","executionInfo":{"status":"ok","timestamp":1602547103242,"user_tz":-780,"elapsed":2706,"user":{"displayName":"Hassaan Mirza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbDdstoeI3uMgQE2uIuTfxDWaRQUODiJ6EY54ABC8=s64","userId":"07902953940759638847"}},"outputId":"86e1157b-0b6e-4bca-9f7f-a5eb16ed853a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e9zIbxPzu4UW","executionInfo":{"status":"ok","timestamp":1602547107309,"user_tz":-780,"elapsed":6767,"user":{"displayName":"Hassaan Mirza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbDdstoeI3uMgQE2uIuTfxDWaRQUODiJ6EY54ABC8=s64","userId":"07902953940759638847"}},"outputId":"95573165-451b-4e60-eba3-b00ee4761963","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["pip install mido"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting mido\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n","\r\u001b[K     |██████▎                         | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n","\u001b[?25hInstalling collected packages: mido\n","Successfully installed mido-1.2.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rbm0oh5Lu9GA","executionInfo":{"status":"ok","timestamp":1602547137914,"user_tz":-780,"elapsed":37366,"user":{"displayName":"Hassaan Mirza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbDdstoeI3uMgQE2uIuTfxDWaRQUODiJ6EY54ABC8=s64","userId":"07902953940759638847"}},"outputId":"a34fdfd5-e7ce-471b-8ae5-8176a21168e3","colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["pip install madmom"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting madmom\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/a3/9f3de3e8068a3606331134d96b84c8db4f7624d6715be8ab3c1f56e6731d/madmom-0.16.1.tar.gz (20.0MB)\n","\u001b[K     |████████████████████████████████| 20.0MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.4 in /usr/local/lib/python3.6/dist-packages (from madmom) (1.18.5)\n","Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from madmom) (1.4.1)\n","Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.6/dist-packages (from madmom) (0.29.21)\n","Requirement already satisfied: mido>=1.2.8 in /usr/local/lib/python3.6/dist-packages (from madmom) (1.2.9)\n","Building wheels for collected packages: madmom\n","  Building wheel for madmom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for madmom: filename=madmom-0.16.1-cp36-cp36m-linux_x86_64.whl size=20940279 sha256=6f91fd9be4fbdfb0e0a54181665765ac5e7a7a754057727ad91d083542347ed9\n","  Stored in directory: /root/.cache/pip/wheels/21/0c/30/e0141aa75fb0a829ba5e1dca2be0860dc98502c1789616637d\n","Successfully built madmom\n","Installing collected packages: madmom\n","Successfully installed madmom-0.16.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uqDuQxOqvPlt","executionInfo":{"status":"ok","timestamp":1602547142414,"user_tz":-780,"elapsed":41861,"user":{"displayName":"Hassaan Mirza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbDdstoeI3uMgQE2uIuTfxDWaRQUODiJ6EY54ABC8=s64","userId":"07902953940759638847"}},"outputId":"a9114253-68a1-4537-a054-a8c38dd94818","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["pip install pretty_midi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pretty_midi\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/8e/63c6e39a7a64623a9cd6aec530070c70827f6f8f40deec938f323d7b1e15/pretty_midi-0.2.9.tar.gz (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 7.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.18.5)\n","Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.2.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pretty_midi) (1.15.0)\n","Building wheels for collected packages: pretty-midi\n","  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-cp36-none-any.whl size=5591954 sha256=1c69cc67d47d856b786203f1defe4cb5c24eb973599e6a9f865c4dd7fe0d9e3d\n","  Stored in directory: /root/.cache/pip/wheels/4c/a1/c6/b5697841db1112c6e5866d75a6b6bf1bef73b874782556ba66\n","Successfully built pretty-midi\n","Installing collected packages: pretty-midi\n","Successfully installed pretty-midi-0.2.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5bkdBA-YvYRV"},"source":["'''\n","\n","keras: CNN Transcription model\n","\n","'''\n","#from __future__ import print_function\n","import argparse\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","#keras utils\n","from keras.callbacks import Callback\n","from keras import metrics\n","from keras.models import Model, load_model\n","from keras.layers import Dense, Dropout, Flatten, Reshape, Input\n","from keras.layers import Conv2D, MaxPooling2D, add\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, CSVLogger\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers import Activation\n","from keras.optimizers import SGD\n","from keras import backend as K\n","from keras.utils import plot_model\n","\n","\n","import tensorflow as tf\n","import sklearn\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","#internal utils\n","from preprocess import DataGen\n","from config import load_config\n","\n","import numpy as np\n","\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AnZ7hAXvc4P","executionInfo":{"status":"ok","timestamp":1602560233034,"user_tz":-780,"elapsed":13132472,"user":{"displayName":"Hassaan Mirza","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbDdstoeI3uMgQE2uIuTfxDWaRQUODiJ6EY54ABC8=s64","userId":"07902953940759638847"}},"outputId":"5914b654-6fa8-47af-ae26-d0ed4ad0a3b8","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python runs.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2020-10-12 23:59:11.008017: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","deleted the directory baseline\n","deleted the directory models\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","tcmalloc: large alloc 2528641024 bytes == 0x1053c000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","tcmalloc: large alloc 2528641024 bytes == 0x1053c000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","tcmalloc: large alloc 3510632448 bytes == 0x1e624000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","tcmalloc: large alloc 2826960896 bytes == 0x25a06000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","tcmalloc: large alloc 2057830400 bytes == 0x28cbe000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","tcmalloc: large alloc 5646057472 bytes == 0x2b7c2000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","tcmalloc: large alloc 5149032448 bytes == 0x3385e000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","tcmalloc: large alloc 4340580352 bytes == 0x44042000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x660c6000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","tcmalloc: large alloc 6211239936 bytes == 0xa0674000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/new/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/new/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","2020-10-13 00:04:00.241817: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n","2020-10-13 00:04:00.309345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:00.310048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n","coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n","2020-10-13 00:04:00.310121: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","2020-10-13 00:04:00.560566: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n","2020-10-13 00:04:00.720358: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n","2020-10-13 00:04:00.749528: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n","2020-10-13 00:04:01.044550: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n","2020-10-13 00:04:01.064324: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n","2020-10-13 00:04:01.577934: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n","2020-10-13 00:04:01.578219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:01.578995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:01.579604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n","2020-10-13 00:04:01.580040: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2020-10-13 00:04:01.594339: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000155000 Hz\n","2020-10-13 00:04:01.594677: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6b7ba40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-10-13 00:04:01.594714: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-10-13 00:04:01.721649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:01.722458: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6b7b880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-10-13 00:04:01.722497: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2020-10-13 00:04:01.723635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:01.724231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n","coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n","2020-10-13 00:04:01.724318: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","2020-10-13 00:04:01.724355: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n","2020-10-13 00:04:01.724376: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n","2020-10-13 00:04:01.724395: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n","2020-10-13 00:04:01.724413: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n","2020-10-13 00:04:01.724431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n","2020-10-13 00:04:01.724473: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n","2020-10-13 00:04:01.724570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:01.725207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:01.725801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n","2020-10-13 00:04:01.728848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","2020-10-13 00:04:05.607524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-10-13 00:04:05.607580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n","2020-10-13 00:04:05.607589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n","2020-10-13 00:04:05.608836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:05.609563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-10-13 00:04:05.610218: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-10-13 00:04:05.610263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14756 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n","0\n","176\n","Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 7, 352, 1)    0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 7, 352, 64)   22592       reshape[0][0]                    \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 7, 176, 64)   0           conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 7, 176, 64)   256         max_pooling2d[0][0]              \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 7, 176, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 7, 176, 64)   720960      activation[0][0]                 \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 7, 176, 64)   0           max_pooling2d[0][0]              \n","                                                                 conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 7, 88, 64)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 39424)        0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1024)         40371200    flatten[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 512)          524800      dropout[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 88)           45144       dropout_1[0][0]                  \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid/keras_train.py:240: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","hd: learning rate is now 0.1\n","Epoch 1/100\n","2020-10-13 00:04:07.492354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n","2020-10-13 00:04:08.992950: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n","  2/360 [..............................] - ETA: 12s - loss: 0.6930WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.0562s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.2389\n","Epoch 00001: val_loss improved from inf to 0.11492, saving model to models/new/ckpt.h5\n","2020-10-13 00:04:41.188231: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 161480704 exceeds 10% of free system memory.\n","2020-10-13 00:04:41.741885: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 161480704 exceeds 10% of free system memory.\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2389 - val_loss: 0.1149\n","hd: learning rate is now 0.1\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1949\n","Epoch 00002: val_loss did not improve from 0.11492\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1949 - val_loss: 0.1181\n","hd: learning rate is now 0.1\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1917\n","Epoch 00003: val_loss improved from 0.11492 to 0.11321, saving model to models/new/ckpt.h5\n","2020-10-13 00:05:33.277287: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 161480704 exceeds 10% of free system memory.\n","2020-10-13 00:05:33.733182: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 161480704 exceeds 10% of free system memory.\n","360/360 [==============================] - 27s 74ms/step - loss: 0.1917 - val_loss: 0.1132\n","hd: learning rate is now 0.1\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1909\n","Epoch 00004: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1909 - val_loss: 0.1310\n","hd: learning rate is now 0.1\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00005: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1526\n","hd: learning rate is now 0.05\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1933\n","Epoch 00006: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1933 - val_loss: 0.1474\n","hd: learning rate is now 0.05\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1913\n","Epoch 00007: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1913 - val_loss: 0.1368\n","hd: learning rate is now 0.05\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1911\n","Epoch 00008: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1911 - val_loss: 0.1318\n","hd: learning rate is now 0.05\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1907\n","Epoch 00009: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1907 - val_loss: 0.1356\n","hd: learning rate is now 0.05\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 00010: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1906 - val_loss: 0.1404\n","hd: learning rate is now 0.025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1915\n","Epoch 00011: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1915 - val_loss: 0.1348\n","hd: learning rate is now 0.025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1908\n","Epoch 00012: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1908 - val_loss: 0.1258\n","hd: learning rate is now 0.025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 00013: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1906 - val_loss: 0.1262\n","hd: learning rate is now 0.025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1903\n","Epoch 00014: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1903 - val_loss: 0.1201\n","hd: learning rate is now 0.025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1903\n","Epoch 00015: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1903 - val_loss: 0.1222\n","hd: learning rate is now 0.0125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1901\n","Epoch 00016: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1901 - val_loss: 0.1190\n","hd: learning rate is now 0.0125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00017: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1175\n","hd: learning rate is now 0.0125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1901\n","Epoch 00018: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1901 - val_loss: 0.1164\n","hd: learning rate is now 0.0125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00019: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1149\n","hd: learning rate is now 0.0125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00020: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1139\n","hd: learning rate is now 0.00625\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1896\n","Epoch 00021: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1896 - val_loss: 0.1154\n","hd: learning rate is now 0.00625\n","Epoch 22/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1894\n","Epoch 00022: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1894 - val_loss: 0.1147\n","hd: learning rate is now 0.00625\n","Epoch 23/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1896\n","Epoch 00023: val_loss did not improve from 0.11321\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1896 - val_loss: 0.1155\n","Epoch 00023: early stopping\n","Directory  baseline  created\n","['loss', 'val_loss']\n","initializing gen for models/new/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","WARNING:tensorflow:From /content/gdrive/Shared drives/Part IV Project Resources/Recreation Notebooks/wav2mid/keras_train.py:276: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.evaluate, which supports generators.\n","['loss']\n","0.2351752668619156\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","tcmalloc: large alloc 5646057472 bytes == 0xa7894000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","tcmalloc: large alloc 5149032448 bytes == 0xb313a000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/newlr/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/newlr/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","0\n","176\n","Model: \"functional_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 7, 352, 1)    0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 7, 352, 64)   22592       reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 7, 176, 64)   0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 7, 176, 64)   256         max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 7, 176, 64)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 7, 176, 64)   720960      activation_1[0][0]               \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 7, 176, 64)   0           max_pooling2d_2[0][0]            \n","                                                                 conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 7, 88, 64)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 39424)        0           max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1024)         40371200    flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 512)          524800      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 88)           45144       dropout_3[0][0]                  \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","hd: learning rate is now 0.001\n","Epoch 1/100\n","  2/360 [..............................] - ETA: 13s - loss: 0.6931WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.0596s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.6550\n","Epoch 00001: val_loss improved from inf to 0.19394, saving model to models/newlr/ckpt.h5\n","2020-10-13 00:18:49.890348: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 161480704 exceeds 10% of free system memory.\n","360/360 [==============================] - 26s 73ms/step - loss: 0.6550 - val_loss: 0.1939\n","hd: learning rate is now 0.001\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2923\n","Epoch 00002: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2923 - val_loss: 0.3347\n","hd: learning rate is now 0.001\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2396\n","Epoch 00003: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2396 - val_loss: 0.3849\n","hd: learning rate is now 0.001\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2283\n","Epoch 00004: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2283 - val_loss: 0.3464\n","hd: learning rate is now 0.001\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2221\n","Epoch 00005: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2221 - val_loss: 0.3045\n","hd: learning rate is now 0.0005\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2194\n","Epoch 00006: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2194 - val_loss: 0.2998\n","hd: learning rate is now 0.0005\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2174\n","Epoch 00007: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2174 - val_loss: 0.2804\n","hd: learning rate is now 0.0005\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2158\n","Epoch 00008: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2158 - val_loss: 0.2633\n","hd: learning rate is now 0.0005\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2139\n","Epoch 00009: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2139 - val_loss: 0.2478\n","hd: learning rate is now 0.0005\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2134\n","Epoch 00010: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2134 - val_loss: 0.2324\n","hd: learning rate is now 0.00025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2127\n","Epoch 00011: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2127 - val_loss: 0.2301\n","hd: learning rate is now 0.00025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2123\n","Epoch 00012: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2123 - val_loss: 0.2239\n","hd: learning rate is now 0.00025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2119\n","Epoch 00013: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2119 - val_loss: 0.2185\n","hd: learning rate is now 0.00025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2112\n","Epoch 00014: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2112 - val_loss: 0.2120\n","hd: learning rate is now 0.00025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2107\n","Epoch 00015: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2107 - val_loss: 0.2064\n","hd: learning rate is now 0.000125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2105\n","Epoch 00016: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2105 - val_loss: 0.2030\n","hd: learning rate is now 0.000125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2102\n","Epoch 00017: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2102 - val_loss: 0.2005\n","hd: learning rate is now 0.000125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2104\n","Epoch 00018: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2104 - val_loss: 0.1978\n","hd: learning rate is now 0.000125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00019: val_loss did not improve from 0.19394\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2101 - val_loss: 0.1954\n","hd: learning rate is now 0.000125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00020: val_loss improved from 0.19394 to 0.19305, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2101 - val_loss: 0.1930\n","hd: learning rate is now 6.25e-05\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00021: val_loss improved from 0.19305 to 0.19199, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2095 - val_loss: 0.1920\n","hd: learning rate is now 6.25e-05\n","Epoch 22/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00022: val_loss improved from 0.19199 to 0.19088, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 28s 77ms/step - loss: 0.2097 - val_loss: 0.1909\n","hd: learning rate is now 6.25e-05\n","Epoch 23/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00023: val_loss improved from 0.19088 to 0.18991, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2095 - val_loss: 0.1899\n","hd: learning rate is now 6.25e-05\n","Epoch 24/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 00024: val_loss improved from 0.18991 to 0.18902, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2094 - val_loss: 0.1890\n","hd: learning rate is now 6.25e-05\n","Epoch 25/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00025: val_loss improved from 0.18902 to 0.18804, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2095 - val_loss: 0.1880\n","hd: learning rate is now 3.125e-05\n","Epoch 26/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00026: val_loss improved from 0.18804 to 0.18752, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2092 - val_loss: 0.1875\n","hd: learning rate is now 3.125e-05\n","Epoch 27/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00027: val_loss improved from 0.18752 to 0.18702, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2090 - val_loss: 0.1870\n","hd: learning rate is now 3.125e-05\n","Epoch 28/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00028: val_loss improved from 0.18702 to 0.18659, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2091 - val_loss: 0.1866\n","hd: learning rate is now 3.125e-05\n","Epoch 29/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00029: val_loss improved from 0.18659 to 0.18605, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2090 - val_loss: 0.1860\n","hd: learning rate is now 3.125e-05\n","Epoch 30/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00030: val_loss improved from 0.18605 to 0.18552, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2090 - val_loss: 0.1855\n","hd: learning rate is now 1.5625e-05\n","Epoch 31/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00031: val_loss improved from 0.18552 to 0.18526, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2091 - val_loss: 0.1853\n","hd: learning rate is now 1.5625e-05\n","Epoch 32/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00032: val_loss improved from 0.18526 to 0.18508, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2089 - val_loss: 0.1851\n","hd: learning rate is now 1.5625e-05\n","Epoch 33/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00033: val_loss improved from 0.18508 to 0.18491, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2091 - val_loss: 0.1849\n","hd: learning rate is now 1.5625e-05\n","Epoch 34/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00034: val_loss improved from 0.18491 to 0.18471, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2091 - val_loss: 0.1847\n","hd: learning rate is now 1.5625e-05\n","Epoch 35/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00035: val_loss improved from 0.18471 to 0.18452, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2089 - val_loss: 0.1845\n","hd: learning rate is now 7.8125e-06\n","Epoch 36/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00036: val_loss improved from 0.18452 to 0.18424, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 28s 76ms/step - loss: 0.2088 - val_loss: 0.1842\n","hd: learning rate is now 7.8125e-06\n","Epoch 37/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00037: val_loss improved from 0.18424 to 0.18411, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2088 - val_loss: 0.1841\n","hd: learning rate is now 7.8125e-06\n","Epoch 38/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2082\n","Epoch 00038: val_loss improved from 0.18411 to 0.18401, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2082 - val_loss: 0.1840\n","hd: learning rate is now 7.8125e-06\n","Epoch 39/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00039: val_loss improved from 0.18401 to 0.18392, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2085 - val_loss: 0.1839\n","hd: learning rate is now 7.8125e-06\n","Epoch 40/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00040: val_loss improved from 0.18392 to 0.18379, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2085 - val_loss: 0.1838\n","hd: learning rate is now 3.90625e-06\n","Epoch 41/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00041: val_loss improved from 0.18379 to 0.18369, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2088 - val_loss: 0.1837\n","hd: learning rate is now 3.90625e-06\n","Epoch 42/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00042: val_loss improved from 0.18369 to 0.18363, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2085 - val_loss: 0.1836\n","hd: learning rate is now 3.90625e-06\n","Epoch 43/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2084\n","Epoch 00043: val_loss improved from 0.18363 to 0.18357, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2084 - val_loss: 0.1836\n","hd: learning rate is now 3.90625e-06\n","Epoch 44/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2083\n","Epoch 00044: val_loss improved from 0.18357 to 0.18350, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2083 - val_loss: 0.1835\n","hd: learning rate is now 3.90625e-06\n","Epoch 45/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00045: val_loss improved from 0.18350 to 0.18343, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2085 - val_loss: 0.1834\n","hd: learning rate is now 1.953125e-06\n","Epoch 46/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2084\n","Epoch 00046: val_loss improved from 0.18343 to 0.18342, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2084 - val_loss: 0.1834\n","hd: learning rate is now 1.953125e-06\n","Epoch 47/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00047: val_loss improved from 0.18342 to 0.18339, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2089 - val_loss: 0.1834\n","hd: learning rate is now 1.953125e-06\n","Epoch 48/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00048: val_loss improved from 0.18339 to 0.18337, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2090 - val_loss: 0.1834\n","hd: learning rate is now 1.953125e-06\n","Epoch 49/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00049: val_loss improved from 0.18337 to 0.18335, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 28s 77ms/step - loss: 0.2089 - val_loss: 0.1833\n","hd: learning rate is now 1.953125e-06\n","Epoch 50/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00050: val_loss improved from 0.18335 to 0.18333, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2087 - val_loss: 0.1833\n","hd: learning rate is now 9.765625e-07\n","Epoch 51/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00051: val_loss improved from 0.18333 to 0.18333, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2086 - val_loss: 0.1833\n","hd: learning rate is now 9.765625e-07\n","Epoch 52/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00052: val_loss improved from 0.18333 to 0.18332, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2088 - val_loss: 0.1833\n","hd: learning rate is now 9.765625e-07\n","Epoch 53/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00053: val_loss improved from 0.18332 to 0.18330, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2090 - val_loss: 0.1833\n","hd: learning rate is now 9.765625e-07\n","Epoch 54/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00054: val_loss improved from 0.18330 to 0.18329, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2087 - val_loss: 0.1833\n","hd: learning rate is now 9.765625e-07\n","Epoch 55/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00055: val_loss improved from 0.18329 to 0.18328, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2089 - val_loss: 0.1833\n","hd: learning rate is now 4.8828125e-07\n","Epoch 56/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00056: val_loss improved from 0.18328 to 0.18328, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2087 - val_loss: 0.1833\n","hd: learning rate is now 4.8828125e-07\n","Epoch 57/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00057: val_loss improved from 0.18328 to 0.18327, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2089 - val_loss: 0.1833\n","hd: learning rate is now 4.8828125e-07\n","Epoch 58/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00058: val_loss improved from 0.18327 to 0.18327, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2088 - val_loss: 0.1833\n","hd: learning rate is now 4.8828125e-07\n","Epoch 59/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00059: val_loss improved from 0.18327 to 0.18326, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2089 - val_loss: 0.1833\n","hd: learning rate is now 4.8828125e-07\n","Epoch 60/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00060: val_loss improved from 0.18326 to 0.18325, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2089 - val_loss: 0.1833\n","hd: learning rate is now 2.44140625e-07\n","Epoch 61/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00061: val_loss improved from 0.18325 to 0.18325, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2087 - val_loss: 0.1833\n","hd: learning rate is now 2.44140625e-07\n","Epoch 62/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00062: val_loss improved from 0.18325 to 0.18325, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 28s 77ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 2.44140625e-07\n","Epoch 63/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00063: val_loss improved from 0.18325 to 0.18325, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2089 - val_loss: 0.1832\n","hd: learning rate is now 2.44140625e-07\n","Epoch 64/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00064: val_loss improved from 0.18325 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 2.44140625e-07\n","Epoch 65/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00065: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 1.220703125e-07\n","Epoch 66/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00066: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 1.220703125e-07\n","Epoch 67/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00067: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 1.220703125e-07\n","Epoch 68/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00068: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 1.220703125e-07\n","Epoch 69/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00069: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 1.220703125e-07\n","Epoch 70/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00070: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 6.103515625e-08\n","Epoch 71/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00071: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 6.103515625e-08\n","Epoch 72/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00072: val_loss improved from 0.18324 to 0.18324, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 6.103515625e-08\n","Epoch 73/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00073: val_loss improved from 0.18324 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 6.103515625e-08\n","Epoch 74/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00074: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 6.103515625e-08\n","Epoch 75/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00075: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 76/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00076: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 77/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00077: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 78/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00078: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 79/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00079: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 80/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00080: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2090 - val_loss: 0.1832\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 81/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00081: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2089 - val_loss: 0.1832\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 82/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00082: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 83/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00083: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 84/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00084: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2085 - val_loss: 0.1832\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 85/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00085: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 86/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00086: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 87/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00087: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 88/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00088: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 89/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00089: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 90/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00090: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 91/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00091: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 92/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00092: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 93/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00093: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2085 - val_loss: 0.1832\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 94/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00094: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2085 - val_loss: 0.1832\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 95/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00095: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 96/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2085\n","Epoch 00096: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2085 - val_loss: 0.1832\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 97/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00097: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2088 - val_loss: 0.1832\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 98/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00098: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2087 - val_loss: 0.1832\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 99/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00099: val_loss did not improve from 0.18323\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2086 - val_loss: 0.1832\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 100/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2087\n","Epoch 00100: val_loss improved from 0.18323 to 0.18323, saving model to models/newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2087 - val_loss: 0.1832\n","Directory  baseline  already exists\n","['loss', 'val_loss']\n","initializing gen for models/newlr/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","['loss']\n","0.2371528446674347\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/short_new/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/short_new/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","0\n","176\n","Model: \"functional_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 7, 352, 1)    0           input_3[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 7, 352, 64)   22592       reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_4 (MaxPooling2D)  (None, 7, 176, 64)   0           conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 176, 64)   256         max_pooling2d_4[0][0]            \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 7, 176, 64)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 7, 176, 64)   720960      activation_2[0][0]               \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 7, 176, 64)   0           max_pooling2d_4[0][0]            \n","                                                                 conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_5 (MaxPooling2D)  (None, 7, 88, 64)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 39424)        0           max_pooling2d_5[0][0]            \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 1024)         40371200    flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 1024)         0           dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 512)          524800      dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 512)          0           dense_7[0][0]                    \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 88)           45144       dropout_5[0][0]                  \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","hd: learning rate is now 0.1\n","Epoch 1/100\n","  2/360 [..............................] - ETA: 13s - loss: 0.6930WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_end` time: 0.0609s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.2377\n","Epoch 00001: val_loss improved from inf to 0.11624, saving model to models/short_new/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2377 - val_loss: 0.1162\n","hd: learning rate is now 0.1\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1948\n","Epoch 00002: val_loss did not improve from 0.11624\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1948 - val_loss: 0.1180\n","hd: learning rate is now 0.1\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1911\n","Epoch 00003: val_loss improved from 0.11624 to 0.11155, saving model to models/short_new/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.1911 - val_loss: 0.1116\n","hd: learning rate is now 0.1\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1907\n","Epoch 00004: val_loss improved from 0.11155 to 0.11014, saving model to models/short_new/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.1907 - val_loss: 0.1101\n","hd: learning rate is now 0.1\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1905\n","Epoch 00005: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1905 - val_loss: 0.1120\n","hd: learning rate is now 0.05\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1932\n","Epoch 00006: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1932 - val_loss: 0.1104\n","hd: learning rate is now 0.05\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1912\n","Epoch 00007: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1912 - val_loss: 0.1103\n","hd: learning rate is now 0.05\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1909\n","Epoch 00008: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1909 - val_loss: 0.1117\n","hd: learning rate is now 0.05\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1904\n","Epoch 00009: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1904 - val_loss: 0.1106\n","hd: learning rate is now 0.05\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1907\n","Epoch 00010: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1907 - val_loss: 0.1148\n","hd: learning rate is now 0.025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1913\n","Epoch 00011: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1913 - val_loss: 0.1132\n","hd: learning rate is now 0.025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 00012: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1906 - val_loss: 0.1138\n","hd: learning rate is now 0.025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1905\n","Epoch 00013: val_loss did not improve from 0.11014\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1905 - val_loss: 0.1123\n","hd: learning rate is now 0.025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1902\n","Epoch 00014: val_loss improved from 0.11014 to 0.10999, saving model to models/short_new/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.1902 - val_loss: 0.1100\n","hd: learning rate is now 0.025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1902\n","Epoch 00015: val_loss improved from 0.10999 to 0.10833, saving model to models/short_new/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.1902 - val_loss: 0.1083\n","hd: learning rate is now 0.0125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00016: val_loss improved from 0.10833 to 0.10794, saving model to models/short_new/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.1900 - val_loss: 0.1079\n","hd: learning rate is now 0.0125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1898\n","Epoch 00017: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1898 - val_loss: 0.1084\n","hd: learning rate is now 0.0125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1899\n","Epoch 00018: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1899 - val_loss: 0.1088\n","hd: learning rate is now 0.0125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1899\n","Epoch 00019: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1899 - val_loss: 0.1093\n","hd: learning rate is now 0.0125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1899\n","Epoch 00020: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1899 - val_loss: 0.1088\n","hd: learning rate is now 0.00625\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1895\n","Epoch 00021: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1895 - val_loss: 0.1082\n","hd: learning rate is now 0.00625\n","Epoch 22/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1894\n","Epoch 00022: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1894 - val_loss: 0.1086\n","hd: learning rate is now 0.00625\n","Epoch 23/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1894\n","Epoch 00023: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1894 - val_loss: 0.1092\n","hd: learning rate is now 0.00625\n","Epoch 24/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1894\n","Epoch 00024: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1894 - val_loss: 0.1095\n","hd: learning rate is now 0.00625\n","Epoch 25/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1893\n","Epoch 00025: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1893 - val_loss: 0.1098\n","hd: learning rate is now 0.003125\n","Epoch 26/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1891\n","Epoch 00026: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1891 - val_loss: 0.1099\n","hd: learning rate is now 0.003125\n","Epoch 27/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1890\n","Epoch 00027: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1890 - val_loss: 0.1100\n","hd: learning rate is now 0.003125\n","Epoch 28/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1890\n","Epoch 00028: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1890 - val_loss: 0.1099\n","hd: learning rate is now 0.003125\n","Epoch 29/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1891\n","Epoch 00029: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1891 - val_loss: 0.1098\n","hd: learning rate is now 0.003125\n","Epoch 30/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1891\n","Epoch 00030: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1891 - val_loss: 0.1098\n","hd: learning rate is now 0.0015625\n","Epoch 31/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1889\n","Epoch 00031: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1889 - val_loss: 0.1099\n","hd: learning rate is now 0.0015625\n","Epoch 32/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1889\n","Epoch 00032: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1889 - val_loss: 0.1099\n","hd: learning rate is now 0.0015625\n","Epoch 33/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1890\n","Epoch 00033: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1890 - val_loss: 0.1100\n","hd: learning rate is now 0.0015625\n","Epoch 34/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1890\n","Epoch 00034: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1890 - val_loss: 0.1100\n","hd: learning rate is now 0.0015625\n","Epoch 35/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1890\n","Epoch 00035: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1890 - val_loss: 0.1100\n","hd: learning rate is now 0.00078125\n","Epoch 36/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1887\n","Epoch 00036: val_loss did not improve from 0.10794\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1887 - val_loss: 0.1101\n","Epoch 00036: early stopping\n","Directory  baseline  already exists\n","['loss', 'val_loss']\n","initializing gen for models/short_new/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","['loss']\n","0.23612205684185028\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/short_newlr/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/short_newlr/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","0\n","176\n","Model: \"functional_7\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 7, 352, 1)    0           input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 7, 352, 64)   22592       reshape_3[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_6 (MaxPooling2D)  (None, 7, 176, 64)   0           conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 7, 176, 64)   256         max_pooling2d_6[0][0]            \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 7, 176, 64)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 7, 176, 64)   720960      activation_3[0][0]               \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 7, 176, 64)   0           max_pooling2d_6[0][0]            \n","                                                                 conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_7 (MaxPooling2D)  (None, 7, 88, 64)    0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","flatten_3 (Flatten)             (None, 39424)        0           max_pooling2d_7[0][0]            \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 1024)         40371200    flatten_3[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 1024)         0           dense_9[0][0]                    \n","__________________________________________________________________________________________________\n","dense_10 (Dense)                (None, 512)          524800      dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n","__________________________________________________________________________________________________\n","dense_11 (Dense)                (None, 88)           45144       dropout_7[0][0]                  \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","hd: learning rate is now 0.001\n","Epoch 1/100\n","  2/360 [..............................] - ETA: 13s - loss: 0.6931WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0608s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.6743\n","Epoch 00001: val_loss improved from inf to 0.48981, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.6743 - val_loss: 0.4898\n","hd: learning rate is now 0.001\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.3432\n","Epoch 00002: val_loss improved from 0.48981 to 0.25241, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.3432 - val_loss: 0.2524\n","hd: learning rate is now 0.001\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2446\n","Epoch 00003: val_loss did not improve from 0.25241\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2446 - val_loss: 0.2673\n","hd: learning rate is now 0.001\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2309\n","Epoch 00004: val_loss improved from 0.25241 to 0.23722, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2309 - val_loss: 0.2372\n","hd: learning rate is now 0.001\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2239\n","Epoch 00005: val_loss improved from 0.23722 to 0.21502, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2239 - val_loss: 0.2150\n","hd: learning rate is now 0.0005\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2206\n","Epoch 00006: val_loss did not improve from 0.21502\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2206 - val_loss: 0.2185\n","hd: learning rate is now 0.0005\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2186\n","Epoch 00007: val_loss improved from 0.21502 to 0.20927, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2186 - val_loss: 0.2093\n","hd: learning rate is now 0.0005\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2168\n","Epoch 00008: val_loss improved from 0.20927 to 0.20450, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2168 - val_loss: 0.2045\n","hd: learning rate is now 0.0005\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2152\n","Epoch 00009: val_loss improved from 0.20450 to 0.19945, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 28s 77ms/step - loss: 0.2152 - val_loss: 0.1995\n","hd: learning rate is now 0.0005\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2141\n","Epoch 00010: val_loss improved from 0.19945 to 0.19507, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2141 - val_loss: 0.1951\n","hd: learning rate is now 0.00025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2134\n","Epoch 00011: val_loss did not improve from 0.19507\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2134 - val_loss: 0.1962\n","hd: learning rate is now 0.00025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2131\n","Epoch 00012: val_loss improved from 0.19507 to 0.19458, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2131 - val_loss: 0.1946\n","hd: learning rate is now 0.00025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2125\n","Epoch 00013: val_loss improved from 0.19458 to 0.19333, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2125 - val_loss: 0.1933\n","hd: learning rate is now 0.00025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2118\n","Epoch 00014: val_loss improved from 0.19333 to 0.19163, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2118 - val_loss: 0.1916\n","hd: learning rate is now 0.00025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2113\n","Epoch 00015: val_loss improved from 0.19163 to 0.19019, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2113 - val_loss: 0.1902\n","hd: learning rate is now 0.000125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2109\n","Epoch 00016: val_loss improved from 0.19019 to 0.18891, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2109 - val_loss: 0.1889\n","hd: learning rate is now 0.000125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2111\n","Epoch 00017: val_loss improved from 0.18891 to 0.18820, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2111 - val_loss: 0.1882\n","hd: learning rate is now 0.000125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2110\n","Epoch 00018: val_loss improved from 0.18820 to 0.18770, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2110 - val_loss: 0.1877\n","hd: learning rate is now 0.000125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2107\n","Epoch 00019: val_loss improved from 0.18770 to 0.18734, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2107 - val_loss: 0.1873\n","hd: learning rate is now 0.000125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2104\n","Epoch 00020: val_loss improved from 0.18734 to 0.18706, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2104 - val_loss: 0.1871\n","hd: learning rate is now 6.25e-05\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2102\n","Epoch 00021: val_loss improved from 0.18706 to 0.18632, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2102 - val_loss: 0.1863\n","hd: learning rate is now 6.25e-05\n","Epoch 22/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00022: val_loss improved from 0.18632 to 0.18604, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1860\n","hd: learning rate is now 6.25e-05\n","Epoch 23/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00023: val_loss improved from 0.18604 to 0.18577, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1858\n","hd: learning rate is now 6.25e-05\n","Epoch 24/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00024: val_loss improved from 0.18577 to 0.18549, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 28s 77ms/step - loss: 0.2101 - val_loss: 0.1855\n","hd: learning rate is now 6.25e-05\n","Epoch 25/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00025: val_loss improved from 0.18549 to 0.18517, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1852\n","hd: learning rate is now 3.125e-05\n","Epoch 26/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00026: val_loss improved from 0.18517 to 0.18491, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1849\n","hd: learning rate is now 3.125e-05\n","Epoch 27/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00027: val_loss improved from 0.18491 to 0.18485, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2096 - val_loss: 0.1848\n","hd: learning rate is now 3.125e-05\n","Epoch 28/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00028: val_loss improved from 0.18485 to 0.18467, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2096 - val_loss: 0.1847\n","hd: learning rate is now 3.125e-05\n","Epoch 29/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00029: val_loss did not improve from 0.18467\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2097 - val_loss: 0.1847\n","hd: learning rate is now 3.125e-05\n","Epoch 30/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00030: val_loss improved from 0.18467 to 0.18457, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2095 - val_loss: 0.1846\n","hd: learning rate is now 1.5625e-05\n","Epoch 31/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00031: val_loss improved from 0.18457 to 0.18448, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2095 - val_loss: 0.1845\n","hd: learning rate is now 1.5625e-05\n","Epoch 32/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 00032: val_loss improved from 0.18448 to 0.18441, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2094 - val_loss: 0.1844\n","hd: learning rate is now 1.5625e-05\n","Epoch 33/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00033: val_loss improved from 0.18441 to 0.18437, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2096 - val_loss: 0.1844\n","hd: learning rate is now 1.5625e-05\n","Epoch 34/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00034: val_loss improved from 0.18437 to 0.18432, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2093 - val_loss: 0.1843\n","hd: learning rate is now 1.5625e-05\n","Epoch 35/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 00035: val_loss improved from 0.18432 to 0.18424, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2094 - val_loss: 0.1842\n","hd: learning rate is now 7.8125e-06\n","Epoch 36/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00036: val_loss improved from 0.18424 to 0.18410, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2092 - val_loss: 0.1841\n","hd: learning rate is now 7.8125e-06\n","Epoch 37/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00037: val_loss did not improve from 0.18410\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2092 - val_loss: 0.1841\n","hd: learning rate is now 7.8125e-06\n","Epoch 38/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2086\n","Epoch 00038: val_loss did not improve from 0.18410\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2086 - val_loss: 0.1841\n","hd: learning rate is now 7.8125e-06\n","Epoch 39/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00039: val_loss improved from 0.18410 to 0.18409, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2090 - val_loss: 0.1841\n","hd: learning rate is now 7.8125e-06\n","Epoch 40/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00040: val_loss improved from 0.18409 to 0.18408, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 28s 77ms/step - loss: 0.2093 - val_loss: 0.1841\n","hd: learning rate is now 3.90625e-06\n","Epoch 41/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00041: val_loss improved from 0.18408 to 0.18404, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2092 - val_loss: 0.1840\n","hd: learning rate is now 3.90625e-06\n","Epoch 42/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00042: val_loss improved from 0.18404 to 0.18403, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2091 - val_loss: 0.1840\n","hd: learning rate is now 3.90625e-06\n","Epoch 43/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2089\n","Epoch 00043: val_loss improved from 0.18403 to 0.18401, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2089 - val_loss: 0.1840\n","hd: learning rate is now 3.90625e-06\n","Epoch 44/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 00044: val_loss improved from 0.18401 to 0.18398, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2088 - val_loss: 0.1840\n","hd: learning rate is now 3.90625e-06\n","Epoch 45/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00045: val_loss did not improve from 0.18398\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2090 - val_loss: 0.1840\n","hd: learning rate is now 1.953125e-06\n","Epoch 46/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00046: val_loss did not improve from 0.18398\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2091 - val_loss: 0.1840\n","hd: learning rate is now 1.953125e-06\n","Epoch 47/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00047: val_loss improved from 0.18398 to 0.18397, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2092 - val_loss: 0.1840\n","hd: learning rate is now 1.953125e-06\n","Epoch 48/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00048: val_loss improved from 0.18397 to 0.18396, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2095 - val_loss: 0.1840\n","hd: learning rate is now 1.953125e-06\n","Epoch 49/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 00049: val_loss improved from 0.18396 to 0.18394, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2094 - val_loss: 0.1839\n","hd: learning rate is now 1.953125e-06\n","Epoch 50/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00050: val_loss improved from 0.18394 to 0.18393, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 9.765625e-07\n","Epoch 51/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00051: val_loss did not improve from 0.18393\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 9.765625e-07\n","Epoch 52/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00052: val_loss improved from 0.18393 to 0.18393, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 9.765625e-07\n","Epoch 53/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00053: val_loss improved from 0.18393 to 0.18392, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 9.765625e-07\n","Epoch 54/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00054: val_loss improved from 0.18392 to 0.18391, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2095 - val_loss: 0.1839\n","hd: learning rate is now 9.765625e-07\n","Epoch 55/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00055: val_loss improved from 0.18391 to 0.18391, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 4.8828125e-07\n","Epoch 56/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00056: val_loss did not improve from 0.18391\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 4.8828125e-07\n","Epoch 57/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00057: val_loss improved from 0.18391 to 0.18391, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 4.8828125e-07\n","Epoch 58/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00058: val_loss improved from 0.18391 to 0.18391, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 4.8828125e-07\n","Epoch 59/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00059: val_loss improved from 0.18391 to 0.18390, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 4.8828125e-07\n","Epoch 60/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00060: val_loss improved from 0.18390 to 0.18390, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 2.44140625e-07\n","Epoch 61/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00061: val_loss did not improve from 0.18390\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 2.44140625e-07\n","Epoch 62/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00062: val_loss improved from 0.18390 to 0.18390, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 2.44140625e-07\n","Epoch 63/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 00063: val_loss improved from 0.18390 to 0.18390, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2094 - val_loss: 0.1839\n","hd: learning rate is now 2.44140625e-07\n","Epoch 64/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2090\n","Epoch 00064: val_loss improved from 0.18390 to 0.18390, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2090 - val_loss: 0.1839\n","hd: learning rate is now 2.44140625e-07\n","Epoch 65/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00065: val_loss improved from 0.18390 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 1.220703125e-07\n","Epoch 66/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00066: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2095 - val_loss: 0.1839\n","hd: learning rate is now 1.220703125e-07\n","Epoch 67/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 00067: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2094 - val_loss: 0.1839\n","hd: learning rate is now 1.220703125e-07\n","Epoch 68/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00068: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 1.220703125e-07\n","Epoch 69/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00069: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 1.220703125e-07\n","Epoch 70/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00070: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 6.103515625e-08\n","Epoch 71/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00071: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 6.103515625e-08\n","Epoch 72/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00072: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 6.103515625e-08\n","Epoch 73/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00073: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 6.103515625e-08\n","Epoch 74/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00074: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 6.103515625e-08\n","Epoch 75/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00075: val_loss improved from 0.18389 to 0.18389, saving model to models/short_newlr/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 76/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00076: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 77/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00077: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2093 - val_loss: 0.1839\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 78/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00078: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 79/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00079: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 80/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00080: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 81/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 00081: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2095 - val_loss: 0.1839\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 82/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00082: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 83/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00083: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 84/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00084: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 85/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00085: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 86/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00086: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 87/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00087: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 88/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00088: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 89/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00089: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 90/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2094\n","Epoch 00090: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2094 - val_loss: 0.1839\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 91/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00091: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 92/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00092: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 93/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2091\n","Epoch 00093: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2091 - val_loss: 0.1839\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 94/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00094: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 95/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2092\n","Epoch 00095: val_loss did not improve from 0.18389\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2092 - val_loss: 0.1839\n","Epoch 00095: early stopping\n","Directory  baseline  already exists\n","['loss', 'val_loss']\n","initializing gen for models/short_newlr/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","['loss']\n","0.23683559894561768\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/new_no_res/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/new_no_res/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","0\n","176\n","Model: \"functional_9\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape_4 (Reshape)             (None, 7, 352, 1)    0           input_5[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 7, 352, 64)   22592       reshape_4[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_8 (MaxPooling2D)  (None, 7, 176, 64)   0           conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 7, 176, 64)   256         max_pooling2d_8[0][0]            \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 7, 176, 64)   0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 7, 176, 64)   720960      activation_4[0][0]               \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 7, 176, 64)   0           max_pooling2d_8[0][0]            \n","                                                                 conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_9 (MaxPooling2D)  (None, 7, 88, 64)    0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","flatten_4 (Flatten)             (None, 39424)        0           max_pooling2d_9[0][0]            \n","__________________________________________________________________________________________________\n","dense_12 (Dense)                (None, 1024)         40371200    flatten_4[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 1024)         0           dense_12[0][0]                   \n","__________________________________________________________________________________________________\n","dense_13 (Dense)                (None, 512)          524800      dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 512)          0           dense_13[0][0]                   \n","__________________________________________________________________________________________________\n","dense_14 (Dense)                (None, 88)           45144       dropout_9[0][0]                  \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","hd: learning rate is now 0.1\n","Epoch 1/100\n","  2/360 [..............................] - ETA: 14s - loss: 0.6930WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.0643s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.2389\n","Epoch 00001: val_loss improved from inf to 0.11517, saving model to models/new_no_res/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2389 - val_loss: 0.1152\n","hd: learning rate is now 0.1\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1951\n","Epoch 00002: val_loss did not improve from 0.11517\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1951 - val_loss: 0.1180\n","hd: learning rate is now 0.1\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1914\n","Epoch 00003: val_loss improved from 0.11517 to 0.11257, saving model to models/new_no_res/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.1914 - val_loss: 0.1126\n","hd: learning rate is now 0.1\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1902\n","Epoch 00004: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1902 - val_loss: 0.1196\n","hd: learning rate is now 0.1\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1903\n","Epoch 00005: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1903 - val_loss: 0.1285\n","hd: learning rate is now 0.05\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1924\n","Epoch 00006: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1924 - val_loss: 0.1382\n","hd: learning rate is now 0.05\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1913\n","Epoch 00007: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1913 - val_loss: 0.1341\n","hd: learning rate is now 0.05\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1907\n","Epoch 00008: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1907 - val_loss: 0.1366\n","hd: learning rate is now 0.05\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 00009: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1906 - val_loss: 0.1302\n","hd: learning rate is now 0.05\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1908\n","Epoch 00010: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1908 - val_loss: 0.1311\n","hd: learning rate is now 0.025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1915\n","Epoch 00011: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1915 - val_loss: 0.1267\n","hd: learning rate is now 0.025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1908\n","Epoch 00012: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1908 - val_loss: 0.1233\n","hd: learning rate is now 0.025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 00013: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1906 - val_loss: 0.1211\n","hd: learning rate is now 0.025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00014: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 69ms/step - loss: 0.1900 - val_loss: 0.1203\n","hd: learning rate is now 0.025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1902\n","Epoch 00015: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1902 - val_loss: 0.1191\n","hd: learning rate is now 0.0125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1901\n","Epoch 00016: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1901 - val_loss: 0.1190\n","hd: learning rate is now 0.0125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00017: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1180\n","hd: learning rate is now 0.0125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00018: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1172\n","hd: learning rate is now 0.0125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00019: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1174\n","hd: learning rate is now 0.0125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00020: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1167\n","hd: learning rate is now 0.00625\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1895\n","Epoch 00021: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1895 - val_loss: 0.1165\n","hd: learning rate is now 0.00625\n","Epoch 22/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1894\n","Epoch 00022: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1894 - val_loss: 0.1165\n","hd: learning rate is now 0.00625\n","Epoch 23/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1895\n","Epoch 00023: val_loss did not improve from 0.11257\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1895 - val_loss: 0.1165\n","Epoch 00023: early stopping\n","Directory  baseline  already exists\n","['loss', 'val_loss']\n","initializing gen for models/new_no_res/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","['loss']\n","0.2355118840932846\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/newlr_nores/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/newlr_nores/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","0\n","176\n","Model: \"functional_11\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_6 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape_5 (Reshape)             (None, 7, 352, 1)    0           input_6[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 7, 352, 64)   22592       reshape_5[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_10 (MaxPooling2D) (None, 7, 176, 64)   0           conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 7, 176, 64)   256         max_pooling2d_10[0][0]           \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 7, 176, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 7, 176, 64)   720960      activation_5[0][0]               \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 7, 176, 64)   0           max_pooling2d_10[0][0]           \n","                                                                 conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_11 (MaxPooling2D) (None, 7, 88, 64)    0           add_5[0][0]                      \n","__________________________________________________________________________________________________\n","flatten_5 (Flatten)             (None, 39424)        0           max_pooling2d_11[0][0]           \n","__________________________________________________________________________________________________\n","dense_15 (Dense)                (None, 1024)         40371200    flatten_5[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_10 (Dropout)            (None, 1024)         0           dense_15[0][0]                   \n","__________________________________________________________________________________________________\n","dense_16 (Dense)                (None, 512)          524800      dropout_10[0][0]                 \n","__________________________________________________________________________________________________\n","dropout_11 (Dropout)            (None, 512)          0           dense_16[0][0]                   \n","__________________________________________________________________________________________________\n","dense_17 (Dense)                (None, 88)           45144       dropout_11[0][0]                 \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","hd: learning rate is now 0.001\n","Epoch 1/100\n","  2/360 [..............................] - ETA: 14s - loss: 0.6931WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.0643s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.6860\n","Epoch 00001: val_loss improved from inf to 0.68034, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.6860 - val_loss: 0.6803\n","hd: learning rate is now 0.001\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.6296\n","Epoch 00002: val_loss improved from 0.68034 to 0.26737, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.6296 - val_loss: 0.2674\n","hd: learning rate is now 0.001\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2900\n","Epoch 00003: val_loss did not improve from 0.26737\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2900 - val_loss: 0.3812\n","hd: learning rate is now 0.001\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2395\n","Epoch 00004: val_loss did not improve from 0.26737\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2395 - val_loss: 0.3451\n","hd: learning rate is now 0.001\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2281\n","Epoch 00005: val_loss did not improve from 0.26737\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2281 - val_loss: 0.3080\n","hd: learning rate is now 0.0005\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2237\n","Epoch 00006: val_loss did not improve from 0.26737\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2237 - val_loss: 0.3066\n","hd: learning rate is now 0.0005\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2208\n","Epoch 00007: val_loss did not improve from 0.26737\n","360/360 [==============================] - 25s 69ms/step - loss: 0.2208 - val_loss: 0.2892\n","hd: learning rate is now 0.0005\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2189\n","Epoch 00008: val_loss did not improve from 0.26737\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2189 - val_loss: 0.2731\n","hd: learning rate is now 0.0005\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2166\n","Epoch 00009: val_loss improved from 0.26737 to 0.25758, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2166 - val_loss: 0.2576\n","hd: learning rate is now 0.0005\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2156\n","Epoch 00010: val_loss improved from 0.25758 to 0.24325, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2156 - val_loss: 0.2432\n","hd: learning rate is now 0.00025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2146\n","Epoch 00011: val_loss improved from 0.24325 to 0.23870, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2146 - val_loss: 0.2387\n","hd: learning rate is now 0.00025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2144\n","Epoch 00012: val_loss improved from 0.23870 to 0.23209, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2144 - val_loss: 0.2321\n","hd: learning rate is now 0.00025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2135\n","Epoch 00013: val_loss improved from 0.23209 to 0.22540, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2135 - val_loss: 0.2254\n","hd: learning rate is now 0.00025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2128\n","Epoch 00014: val_loss improved from 0.22540 to 0.21874, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2128 - val_loss: 0.2187\n","hd: learning rate is now 0.00025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2121\n","Epoch 00015: val_loss improved from 0.21874 to 0.21278, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2121 - val_loss: 0.2128\n","hd: learning rate is now 0.000125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2119\n","Epoch 00016: val_loss improved from 0.21278 to 0.20932, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2119 - val_loss: 0.2093\n","hd: learning rate is now 0.000125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2116\n","Epoch 00017: val_loss improved from 0.20932 to 0.20636, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2116 - val_loss: 0.2064\n","hd: learning rate is now 0.000125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2116\n","Epoch 00018: val_loss improved from 0.20636 to 0.20338, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2116 - val_loss: 0.2034\n","hd: learning rate is now 0.000125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2113\n","Epoch 00019: val_loss improved from 0.20338 to 0.20057, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2113 - val_loss: 0.2006\n","hd: learning rate is now 0.000125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2113\n","Epoch 00020: val_loss improved from 0.20057 to 0.19810, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2113 - val_loss: 0.1981\n","hd: learning rate is now 6.25e-05\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2108\n","Epoch 00021: val_loss improved from 0.19810 to 0.19654, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2108 - val_loss: 0.1965\n","hd: learning rate is now 6.25e-05\n","Epoch 22/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2107\n","Epoch 00022: val_loss improved from 0.19654 to 0.19519, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2107 - val_loss: 0.1952\n","hd: learning rate is now 6.25e-05\n","Epoch 23/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2108\n","Epoch 00023: val_loss improved from 0.19519 to 0.19388, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2108 - val_loss: 0.1939\n","hd: learning rate is now 6.25e-05\n","Epoch 24/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2107\n","Epoch 00024: val_loss improved from 0.19388 to 0.19255, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2107 - val_loss: 0.1925\n","hd: learning rate is now 6.25e-05\n","Epoch 25/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2107\n","Epoch 00025: val_loss improved from 0.19255 to 0.19141, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2107 - val_loss: 0.1914\n","hd: learning rate is now 3.125e-05\n","Epoch 26/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2104\n","Epoch 00026: val_loss improved from 0.19141 to 0.19079, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2104 - val_loss: 0.1908\n","hd: learning rate is now 3.125e-05\n","Epoch 27/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2103\n","Epoch 00027: val_loss improved from 0.19079 to 0.19027, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2103 - val_loss: 0.1903\n","hd: learning rate is now 3.125e-05\n","Epoch 28/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00028: val_loss improved from 0.19027 to 0.18975, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2101 - val_loss: 0.1897\n","hd: learning rate is now 3.125e-05\n","Epoch 29/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2103\n","Epoch 00029: val_loss improved from 0.18975 to 0.18916, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2103 - val_loss: 0.1892\n","hd: learning rate is now 3.125e-05\n","Epoch 30/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2104\n","Epoch 00030: val_loss improved from 0.18916 to 0.18864, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2104 - val_loss: 0.1886\n","hd: learning rate is now 1.5625e-05\n","Epoch 31/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00031: val_loss improved from 0.18864 to 0.18823, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2101 - val_loss: 0.1882\n","hd: learning rate is now 1.5625e-05\n","Epoch 32/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2102\n","Epoch 00032: val_loss improved from 0.18823 to 0.18802, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2102 - val_loss: 0.1880\n","hd: learning rate is now 1.5625e-05\n","Epoch 33/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2102\n","Epoch 00033: val_loss improved from 0.18802 to 0.18776, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2102 - val_loss: 0.1878\n","hd: learning rate is now 1.5625e-05\n","Epoch 34/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00034: val_loss improved from 0.18776 to 0.18749, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2100 - val_loss: 0.1875\n","hd: learning rate is now 1.5625e-05\n","Epoch 35/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00035: val_loss improved from 0.18749 to 0.18720, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2101 - val_loss: 0.1872\n","hd: learning rate is now 7.8125e-06\n","Epoch 36/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00036: val_loss improved from 0.18720 to 0.18698, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1870\n","hd: learning rate is now 7.8125e-06\n","Epoch 37/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00037: val_loss improved from 0.18698 to 0.18688, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2099 - val_loss: 0.1869\n","hd: learning rate is now 7.8125e-06\n","Epoch 38/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 00038: val_loss improved from 0.18688 to 0.18676, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2093 - val_loss: 0.1868\n","hd: learning rate is now 7.8125e-06\n","Epoch 39/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00039: val_loss improved from 0.18676 to 0.18663, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2098 - val_loss: 0.1866\n","hd: learning rate is now 7.8125e-06\n","Epoch 40/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00040: val_loss improved from 0.18663 to 0.18648, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1865\n","hd: learning rate is now 3.90625e-06\n","Epoch 41/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00041: val_loss improved from 0.18648 to 0.18640, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1864\n","hd: learning rate is now 3.90625e-06\n","Epoch 42/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00042: val_loss improved from 0.18640 to 0.18633, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2097 - val_loss: 0.1863\n","hd: learning rate is now 3.90625e-06\n","Epoch 43/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00043: val_loss improved from 0.18633 to 0.18627, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2096 - val_loss: 0.1863\n","hd: learning rate is now 3.90625e-06\n","Epoch 44/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00044: val_loss improved from 0.18627 to 0.18621, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2096 - val_loss: 0.1862\n","hd: learning rate is now 3.90625e-06\n","Epoch 45/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00045: val_loss improved from 0.18621 to 0.18615, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2097 - val_loss: 0.1862\n","hd: learning rate is now 1.953125e-06\n","Epoch 46/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00046: val_loss improved from 0.18615 to 0.18613, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2096 - val_loss: 0.1861\n","hd: learning rate is now 1.953125e-06\n","Epoch 47/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00047: val_loss improved from 0.18613 to 0.18610, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1861\n","hd: learning rate is now 1.953125e-06\n","Epoch 48/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00048: val_loss improved from 0.18610 to 0.18607, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2101 - val_loss: 0.1861\n","hd: learning rate is now 1.953125e-06\n","Epoch 49/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00049: val_loss improved from 0.18607 to 0.18604, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2098 - val_loss: 0.1860\n","hd: learning rate is now 1.953125e-06\n","Epoch 50/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00050: val_loss improved from 0.18604 to 0.18601, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 28s 76ms/step - loss: 0.2098 - val_loss: 0.1860\n","hd: learning rate is now 9.765625e-07\n","Epoch 51/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00051: val_loss improved from 0.18601 to 0.18601, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2098 - val_loss: 0.1860\n","hd: learning rate is now 9.765625e-07\n","Epoch 52/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00052: val_loss improved from 0.18601 to 0.18599, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2099 - val_loss: 0.1860\n","hd: learning rate is now 9.765625e-07\n","Epoch 53/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00053: val_loss improved from 0.18599 to 0.18598, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2099 - val_loss: 0.1860\n","hd: learning rate is now 9.765625e-07\n","Epoch 54/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00054: val_loss improved from 0.18598 to 0.18596, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1860\n","hd: learning rate is now 9.765625e-07\n","Epoch 55/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00055: val_loss improved from 0.18596 to 0.18595, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2098 - val_loss: 0.1860\n","hd: learning rate is now 4.8828125e-07\n","Epoch 56/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00056: val_loss improved from 0.18595 to 0.18595, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2099 - val_loss: 0.1860\n","hd: learning rate is now 4.8828125e-07\n","Epoch 57/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00057: val_loss improved from 0.18595 to 0.18595, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 4.8828125e-07\n","Epoch 58/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00058: val_loss improved from 0.18595 to 0.18594, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 4.8828125e-07\n","Epoch 59/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00059: val_loss improved from 0.18594 to 0.18593, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 4.8828125e-07\n","Epoch 60/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00060: val_loss improved from 0.18593 to 0.18593, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 2.44140625e-07\n","Epoch 61/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00061: val_loss improved from 0.18593 to 0.18593, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 2.44140625e-07\n","Epoch 62/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00062: val_loss improved from 0.18593 to 0.18592, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 2.44140625e-07\n","Epoch 63/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2103\n","Epoch 00063: val_loss improved from 0.18592 to 0.18592, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2103 - val_loss: 0.1859\n","hd: learning rate is now 2.44140625e-07\n","Epoch 64/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00064: val_loss improved from 0.18592 to 0.18592, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2097 - val_loss: 0.1859\n","hd: learning rate is now 2.44140625e-07\n","Epoch 65/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00065: val_loss improved from 0.18592 to 0.18592, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2097 - val_loss: 0.1859\n","hd: learning rate is now 1.220703125e-07\n","Epoch 66/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00066: val_loss improved from 0.18592 to 0.18592, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 1.220703125e-07\n","Epoch 67/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00067: val_loss improved from 0.18592 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 1.220703125e-07\n","Epoch 68/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00068: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 73ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 1.220703125e-07\n","Epoch 69/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00069: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2098 - val_loss: 0.1859\n","hd: learning rate is now 1.220703125e-07\n","Epoch 70/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00070: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2097 - val_loss: 0.1859\n","hd: learning rate is now 6.103515625e-08\n","Epoch 71/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00071: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 6.103515625e-08\n","Epoch 72/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00072: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 6.103515625e-08\n","Epoch 73/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00073: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 6.103515625e-08\n","Epoch 74/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00074: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 6.103515625e-08\n","Epoch 75/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00075: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2097 - val_loss: 0.1859\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 76/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00076: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 77/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00077: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 78/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00078: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 79/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00079: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 3.0517578125e-08\n","Epoch 80/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00080: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 81/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00081: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 26s 74ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 82/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00082: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2097 - val_loss: 0.1859\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 83/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00083: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 84/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00084: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 1.52587890625e-08\n","Epoch 85/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00085: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2096 - val_loss: 0.1859\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 86/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00086: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 87/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00087: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 88/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00088: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 89/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00089: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2098 - val_loss: 0.1859\n","hd: learning rate is now 7.62939453125e-09\n","Epoch 90/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00090: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 91/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00091: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2098 - val_loss: 0.1859\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 92/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00092: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 93/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00093: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2098 - val_loss: 0.1859\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 94/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00094: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 75ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 3.814697265625e-09\n","Epoch 95/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00095: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2099 - val_loss: 0.1859\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 96/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00096: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 76ms/step - loss: 0.2098 - val_loss: 0.1859\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 97/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 00097: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2100 - val_loss: 0.1859\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 98/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00098: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 28s 77ms/step - loss: 0.2098 - val_loss: 0.1859\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 99/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2097\n","Epoch 00099: val_loss improved from 0.18591 to 0.18591, saving model to models/newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2097 - val_loss: 0.1859\n","hd: learning rate is now 1.9073486328125e-09\n","Epoch 100/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 00100: val_loss did not improve from 0.18591\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2098 - val_loss: 0.1859\n","Directory  baseline  already exists\n","['loss', 'val_loss']\n","initializing gen for models/newlr_nores/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","['loss']\n","0.23755930364131927\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/short_new_nores/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/short_new_nores/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","0\n","176\n","Model: \"functional_13\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape_6 (Reshape)             (None, 7, 352, 1)    0           input_7[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 7, 352, 64)   22592       reshape_6[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_12 (MaxPooling2D) (None, 7, 176, 64)   0           conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 7, 176, 64)   256         max_pooling2d_12[0][0]           \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 7, 176, 64)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 7, 176, 64)   720960      activation_6[0][0]               \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 7, 176, 64)   0           max_pooling2d_12[0][0]           \n","                                                                 conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_13 (MaxPooling2D) (None, 7, 88, 64)    0           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","flatten_6 (Flatten)             (None, 39424)        0           max_pooling2d_13[0][0]           \n","__________________________________________________________________________________________________\n","dense_18 (Dense)                (None, 1024)         40371200    flatten_6[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_12 (Dropout)            (None, 1024)         0           dense_18[0][0]                   \n","__________________________________________________________________________________________________\n","dense_19 (Dense)                (None, 512)          524800      dropout_12[0][0]                 \n","__________________________________________________________________________________________________\n","dropout_13 (Dropout)            (None, 512)          0           dense_19[0][0]                   \n","__________________________________________________________________________________________________\n","dense_20 (Dense)                (None, 88)           45144       dropout_13[0][0]                 \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","hd: learning rate is now 0.1\n","Epoch 1/100\n","  2/360 [..............................] - ETA: 14s - loss: 0.6930WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0647s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.2390\n","Epoch 00001: val_loss improved from inf to 0.11280, saving model to models/short_new_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.2390 - val_loss: 0.1128\n","hd: learning rate is now 0.1\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1950\n","Epoch 00002: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1950 - val_loss: 0.1227\n","hd: learning rate is now 0.1\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1913\n","Epoch 00003: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1913 - val_loss: 0.1140\n","hd: learning rate is now 0.1\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1901\n","Epoch 00004: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1901 - val_loss: 0.1195\n","hd: learning rate is now 0.1\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1905\n","Epoch 00005: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1905 - val_loss: 0.1281\n","hd: learning rate is now 0.05\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1933\n","Epoch 00006: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1933 - val_loss: 0.1236\n","hd: learning rate is now 0.05\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 00007: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1906 - val_loss: 0.1257\n","hd: learning rate is now 0.05\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1896\n","Epoch 00008: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1896 - val_loss: 0.1267\n","hd: learning rate is now 0.05\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1898\n","Epoch 00009: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1898 - val_loss: 0.1231\n","hd: learning rate is now 0.05\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1908\n","Epoch 00010: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1908 - val_loss: 0.1241\n","hd: learning rate is now 0.025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1913\n","Epoch 00011: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1913 - val_loss: 0.1196\n","hd: learning rate is now 0.025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1909\n","Epoch 00012: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1909 - val_loss: 0.1185\n","hd: learning rate is now 0.025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 00013: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1906 - val_loss: 0.1184\n","hd: learning rate is now 0.025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1902\n","Epoch 00014: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1902 - val_loss: 0.1173\n","hd: learning rate is now 0.025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1903\n","Epoch 00015: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1903 - val_loss: 0.1182\n","hd: learning rate is now 0.0125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00016: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1175\n","hd: learning rate is now 0.0125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1899\n","Epoch 00017: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1899 - val_loss: 0.1184\n","hd: learning rate is now 0.0125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00018: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1184\n","hd: learning rate is now 0.0125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1899\n","Epoch 00019: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1899 - val_loss: 0.1180\n","hd: learning rate is now 0.0125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 00020: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1900 - val_loss: 0.1184\n","hd: learning rate is now 0.00625\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.1895\n","Epoch 00021: val_loss did not improve from 0.11280\n","360/360 [==============================] - 25s 70ms/step - loss: 0.1895 - val_loss: 0.1184\n","Epoch 00021: early stopping\n","Directory  baseline  already exists\n","['loss', 'val_loss']\n","initializing gen for models/short_new_nores/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","['loss']\n","0.2353917360305786\n","./training_data/sub_directory\n","wav2inputnp\n","3.492269384692782e-07 8.158917680245446 0.12253136041648932\n","adding to dataset fprefix Avril14th\n","framecnt is 4824\n","wav2inputnp\n","0.0 13.072254581911771 0.10816174037959854\n","adding to dataset fprefix Bounce\n","framecnt is 5135\n","2 examples in dataset\n","0 examples couldnt be processed\n","./training_data/ENS\n","wav2inputnp\n","0.0 1.2544505743255971 0.022159031791411137\n","adding to dataset fprefix MAPS_MUS-chpn-p12_AkPnBcht\n","framecnt is 7682\n","wav2inputnp\n","0.0 0.6397850130015575 0.006499376400797911\n","adding to dataset fprefix MAPS_MUS-chpn-p13_AkPnBcht\n","framecnt is 14379\n","wav2inputnp\n","0.0 1.6971981910119398 0.026544207707504215\n","adding to dataset fprefix MAPS_MUS-chpn-p14_AkPnBcht\n","framecnt is 15658\n","wav2inputnp\n","0.0 1.162425192802229 0.017275832552232525\n","adding to dataset fprefix MAPS_MUS-chpn-p24_AkPnBcht\n","framecnt is 21051\n","wav2inputnp\n","0.0 1.5287553368213922 0.01954714778909569\n","adding to dataset fprefix MAPS_MUS-chpn-p8_AkPnBcht\n","framecnt is 24977\n","5 examples in dataset\n","0 examples couldnt be processed\n","./training_data/MUS\n","wav2inputnp\n","0.0 1.319475803602324 0.011253215674490658\n","adding to dataset fprefix MAPS_MUS-alb_se3_AkPnBcht\n","framecnt is 35747\n","wav2inputnp\n","0.0 0.9243113194775204 0.008582369241267835\n","adding to dataset fprefix MAPS_MUS-bach_846_AkPnBcht\n","framecnt is 45569\n","wav2inputnp\n","0.0 1.1093662787361869 0.014136666784707092\n","adding to dataset fprefix MAPS_MUS-bach_847_AkPnBcht\n","framecnt is 53849\n","wav2inputnp\n","0.0 1.9985608825623082 0.010596886265371794\n","tcmalloc: large alloc 8937013248 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-bk_xmas5_AkPnBcht\n","framecnt is 70896\n","wav2inputnp\n","0.0 1.4543300893311741 0.01222716338998275\n","tcmalloc: large alloc 11663310848 bytes == 0x2ca440000 @  0x7faa395e2001 0x7faa362cc765 0x7faa36330bb0 0x7faa36332a4f 0x7faa363c9048 0x50a7f5 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x50b053 0x634dd2\n","adding to dataset fprefix MAPS_MUS-chp_op31_AkPnBcht\n","framecnt is 93143\n","wav2inputnp\n","0.0 1.338766831747445 0.01669355469250433\n","adding to dataset fprefix MAPS_MUS-chpn-p1_AkPnBcht\n","framecnt is 94574\n","wav2inputnp\n","0.0 0.9028541232769829 0.012692900013652208\n","adding to dataset fprefix MAPS_MUS-chpn-p3_AkPnBcht\n","framecnt is 96884\n","wav2inputnp\n","0.0 0.9008347268838373 0.005876850981317502\n","adding to dataset fprefix MAPS_MUS-chpn-p4_AkPnBcht\n","framecnt is 101375\n","wav2inputnp\n","0.0 0.8176883871589057 0.008310274294914108\n","adding to dataset fprefix MAPS_MUS-chpn_op25_e2_AkPnBcht\n","framecnt is 105306\n","wav2inputnp\n","0.0 1.065114454272077 0.010923422091924697\n","adding to dataset fprefix MAPS_MUS-chpn_op66_AkPnBcht\n","framecnt is 117154\n","10 examples in dataset\n","0 examples couldnt be processed\n","-------- training --------\n","('bin multiple', '2.0')\n","initializing gen for models/short_newlr_nores/data/train\n","MUS\n","starting with  ['MUS']\n","set inputs,outputs\n","initializing gen for models/short_newlr_nores/data/val\n","sub_directory\n","starting with  ['sub_directory']\n","set inputs,outputs\n","training new model from scratch\n","0\n","176\n","Model: \"functional_15\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_8 (InputLayer)            [(None, 7, 352)]     0                                            \n","__________________________________________________________________________________________________\n","reshape_7 (Reshape)             (None, 7, 352, 1)    0           input_8[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 7, 352, 64)   22592       reshape_7[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_14 (MaxPooling2D) (None, 7, 176, 64)   0           conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 7, 176, 64)   256         max_pooling2d_14[0][0]           \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 7, 176, 64)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 7, 176, 64)   720960      activation_7[0][0]               \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 7, 176, 64)   0           max_pooling2d_14[0][0]           \n","                                                                 conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_15 (MaxPooling2D) (None, 7, 88, 64)    0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","flatten_7 (Flatten)             (None, 39424)        0           max_pooling2d_15[0][0]           \n","__________________________________________________________________________________________________\n","dense_21 (Dense)                (None, 1024)         40371200    flatten_7[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_14 (Dropout)            (None, 1024)         0           dense_21[0][0]                   \n","__________________________________________________________________________________________________\n","dense_22 (Dense)                (None, 512)          524800      dropout_14[0][0]                 \n","__________________________________________________________________________________________________\n","dropout_15 (Dropout)            (None, 512)          0           dense_22[0][0]                   \n","__________________________________________________________________________________________________\n","dense_23 (Dense)                (None, 88)           45144       dropout_15[0][0]                 \n","==================================================================================================\n","Total params: 41,684,952\n","Trainable params: 41,684,824\n","Non-trainable params: 128\n","__________________________________________________________________________________________________\n","hd: learning rate is now 0.001\n","Epoch 1/100\n","  2/360 [..............................] - ETA: 13s - loss: 0.6931WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0626s). Check your callbacks.\n","360/360 [==============================] - ETA: 0s - loss: 0.6248\n","Epoch 00001: val_loss improved from inf to 0.13569, saving model to models/short_newlr_nores/ckpt.h5\n","360/360 [==============================] - 27s 74ms/step - loss: 0.6248 - val_loss: 0.1357\n","hd: learning rate is now 0.001\n","Epoch 2/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2789\n","Epoch 00002: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2789 - val_loss: 0.2906\n","hd: learning rate is now 0.001\n","Epoch 3/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2389\n","Epoch 00003: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2389 - val_loss: 0.3247\n","hd: learning rate is now 0.001\n","Epoch 4/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2279\n","Epoch 00004: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2279 - val_loss: 0.2890\n","hd: learning rate is now 0.001\n","Epoch 5/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2219\n","Epoch 00005: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2219 - val_loss: 0.2491\n","hd: learning rate is now 0.0005\n","Epoch 6/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2195\n","Epoch 00006: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2195 - val_loss: 0.2463\n","hd: learning rate is now 0.0005\n","Epoch 7/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2171\n","Epoch 00007: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2171 - val_loss: 0.2288\n","hd: learning rate is now 0.0005\n","Epoch 8/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2156\n","Epoch 00008: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2156 - val_loss: 0.2143\n","hd: learning rate is now 0.0005\n","Epoch 9/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2141\n","Epoch 00009: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2141 - val_loss: 0.2024\n","hd: learning rate is now 0.0005\n","Epoch 10/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2132\n","Epoch 00010: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2132 - val_loss: 0.1904\n","hd: learning rate is now 0.00025\n","Epoch 11/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2125\n","Epoch 00011: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2125 - val_loss: 0.1906\n","hd: learning rate is now 0.00025\n","Epoch 12/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2124\n","Epoch 00012: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2124 - val_loss: 0.1859\n","hd: learning rate is now 0.00025\n","Epoch 13/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2118\n","Epoch 00013: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2118 - val_loss: 0.1817\n","hd: learning rate is now 0.00025\n","Epoch 14/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2112\n","Epoch 00014: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2112 - val_loss: 0.1781\n","hd: learning rate is now 0.00025\n","Epoch 15/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2108\n","Epoch 00015: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2108 - val_loss: 0.1756\n","hd: learning rate is now 0.000125\n","Epoch 16/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2104\n","Epoch 00016: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2104 - val_loss: 0.1731\n","hd: learning rate is now 0.000125\n","Epoch 17/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2103\n","Epoch 00017: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2103 - val_loss: 0.1716\n","hd: learning rate is now 0.000125\n","Epoch 18/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2104\n","Epoch 00018: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2104 - val_loss: 0.1697\n","hd: learning rate is now 0.000125\n","Epoch 19/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2101\n","Epoch 00019: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2101 - val_loss: 0.1683\n","hd: learning rate is now 0.000125\n","Epoch 20/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2099\n","Epoch 00020: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2099 - val_loss: 0.1669\n","hd: learning rate is now 6.25e-05\n","Epoch 21/100\n","360/360 [==============================] - ETA: 0s - loss: 0.2096\n","Epoch 00021: val_loss did not improve from 0.13569\n","360/360 [==============================] - 25s 70ms/step - loss: 0.2096 - val_loss: 0.1656\n","Epoch 00021: early stopping\n","Directory  baseline  already exists\n","['loss', 'val_loss']\n","initializing gen for models/short_newlr_nores/data/test\n","ENS\n","starting with  ['ENS']\n","set inputs,outputs\n","['loss']\n","0.23646993935108185\n"],"name":"stdout"}]}]}